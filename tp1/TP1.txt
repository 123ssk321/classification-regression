Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the datasets provided, explain the need to standardize the attribute values.
R1: We standardize the attribute values so that there is no outlier attribute value that can deeply affect the predictions made by our model.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: The parameters for standardization are calculated on the train set using StandardScaler's fit_transform and then applied on the test set using just transform


Q3: Classification: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: After splitting the feature columns from the true class column and the k-folds, we obtain the prior probability of an example belonging to a class by dividing the number of examples beloging to that class by the total number of examples.
Code: self.log_priors = [np.log(y[y == cls].shape[0] / y.shape[0]) for cls in self.classes] where y is a 1d array with the class of each example and classes == [0, 1].


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: Our Naïve Bayes classifier predicts the class to which a test example belongs by obtaining the argument(the class) that maximizes the sum of the log prior probability of the example belonging to that class with the sum of the log-likelihood of each feature of that example with respect to the argument.
Code: for cls in self.classes:
        totals.append(self.log_priors[cls] + np.sum([kde.score_samples(x.reshape(1, 1))
                                                    for kde, x in zip(self.kdes[cls], row)]))
      preds.append(np.argmax(totals, axis=0))


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: Initially an increase in the bandwidth gives a big increase in the training error and big decrease in the validation error. But, after certain bandwidths the decrease of the validation error gets lower than the increase of the training error, that is, smaller than initially.


Q6: Explain how you determined the best bandwidth parameter for your classifier. You may include a relevant piece of your code if this helps you explain.
R6: We used 5-fold cross validation technique to determine the best bandwidth, which corresponds to the bandwidth of the Naive Bayes classifier with the smallest validation error.


Q7: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R7: The best hypothesis for the Naive Bayes classifier was obtained using 5-fold cross validation and choosing the model with the smallest validation error. For the Gaussian Naive Bayes there was only one hypothesis so that is the best hypothesis.


Q8: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the one provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R8: best bandwidth = 0.14; best naive bayes true error = 0.06; gaussian naive bayes true error = 0.09; naive bayes error range = 81 +/- 17.06; gaussian naive bayes error range = 118 +/ 20.26; McNemar test values(naive bayes vs gaussian naive bayes) = 5.07.
By the approximate normal test we conclude that the naive bayes classifier has a lower error range than the gaussian naive bayes classifier, which tells us that it is a better classifier than the gaussian naive bayes. The McNemar's test returns 5.07 > 3.84 which further strengthens the conclusion
obtained from the approximate normal test, that is, the naive bayes classifier is better than the gaussian naive bayes classifier.


Q9: Regression: Explain what experiments and plots gave you good
evidence to choose a given model degree. 
R9: The plot of the validation error across the different model degrees helped us compare the validation error of the different model, to choose the one with the lowest validation error. The plot of the true (x-axis) versus the predicted (y-axis) values of the miss distance also contributed to our choice
because we could see how good or bad the models predicted the target values.


Q10: In the case of your mean squared error plot explain why one of the error
curves is always decreasing, while the other is not.
R10: The training error is always decreasing because using a higher degree polynomial model makes the model overfit to the training data, which in turn leads to a bad generalization over unseen data increasing the generalization error for the validation dataset.


Q11: In the plots of the true versus predicted values, where would be
all the points when predicted by an ideal regressor? Justify.
R11: The ideal regressor should make it so that the predicted values are equal to the real values, making a perfect line of slope 1 that passes the origin


Q12: Explain your validation procedure and comment on the true error
of your chosen model for unseen data.
R12: We split the data into 3 sets, the training set for training the different models, the validation set for obtaining the error of each model on unseen data that contributes to choose the best model and the test set to obtain an unbiased prediction of the true error. Our test error is around 3%,
we think is a good error to have.


